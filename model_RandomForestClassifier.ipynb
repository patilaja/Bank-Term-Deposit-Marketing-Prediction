{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RandomForest Classifier Modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update sklearn to prevent version mismatches\n",
    "#!pip install sklearn --upgrade\n",
    "#! pip install shap\n",
    "# Install XGBoost \n",
    "#!pip install lightgbm --upgrade"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read the CSV and Perform Basic Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import depedencies\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load libraries\n",
    "from sklearn.model_selection import train_test_split \n",
    "from sklearn.metrics import classification_report,confusion_matrix,accuracy_score\n",
    "from sklearn.ensemble import RandomForestClassifier,RandomForestRegressor\n",
    "from sklearn.inspection import permutation_importance\n",
    "import shap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# File path \n",
    "data_feature_file = os.path.join(\"\",\"data\",\"featureData\",\"feature_dataframe.csv\")\n",
    "model_result = os.path.join(\"\",\"data\",\"results\",\"RandomForest.csv\")\n",
    "model_file_name = os.path.join(\"\",\"data\",\"model\",'randomForest_finalized_model.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read data\n",
    "df = pd.read_csv(data_feature_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>job</th>\n",
       "      <th>education</th>\n",
       "      <th>contact</th>\n",
       "      <th>duration</th>\n",
       "      <th>campaign</th>\n",
       "      <th>emp.var.rate</th>\n",
       "      <th>cons.price.idx</th>\n",
       "      <th>cons.conf.idx</th>\n",
       "      <th>euribor3m</th>\n",
       "      <th>...</th>\n",
       "      <th>jun</th>\n",
       "      <th>mar</th>\n",
       "      <th>may</th>\n",
       "      <th>nov</th>\n",
       "      <th>oct</th>\n",
       "      <th>sep</th>\n",
       "      <th>mon</th>\n",
       "      <th>thu</th>\n",
       "      <th>tue</th>\n",
       "      <th>wed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>56</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>261</td>\n",
       "      <td>1</td>\n",
       "      <td>1.1</td>\n",
       "      <td>93.994</td>\n",
       "      <td>-36.4</td>\n",
       "      <td>4.857</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>57</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>149</td>\n",
       "      <td>1</td>\n",
       "      <td>1.1</td>\n",
       "      <td>93.994</td>\n",
       "      <td>-36.4</td>\n",
       "      <td>4.857</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>37</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>226</td>\n",
       "      <td>1</td>\n",
       "      <td>1.1</td>\n",
       "      <td>93.994</td>\n",
       "      <td>-36.4</td>\n",
       "      <td>4.857</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>40</td>\n",
       "      <td>10</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>151</td>\n",
       "      <td>1</td>\n",
       "      <td>1.1</td>\n",
       "      <td>93.994</td>\n",
       "      <td>-36.4</td>\n",
       "      <td>4.857</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>56</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>307</td>\n",
       "      <td>1</td>\n",
       "      <td>1.1</td>\n",
       "      <td>93.994</td>\n",
       "      <td>-36.4</td>\n",
       "      <td>4.857</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 36 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   age  job  education  contact  duration  campaign  emp.var.rate  \\\n",
       "0   56    4          2        0       261         1           1.1   \n",
       "1   57    7          5        0       149         1           1.1   \n",
       "2   37    7          5        0       226         1           1.1   \n",
       "3   40   10          3        0       151         1           1.1   \n",
       "4   56    7          5        0       307         1           1.1   \n",
       "\n",
       "   cons.price.idx  cons.conf.idx  euribor3m  ...  jun  mar  may  nov  oct  \\\n",
       "0          93.994          -36.4      4.857  ...    0    0    1    0    0   \n",
       "1          93.994          -36.4      4.857  ...    0    0    1    0    0   \n",
       "2          93.994          -36.4      4.857  ...    0    0    1    0    0   \n",
       "3          93.994          -36.4      4.857  ...    0    0    1    0    0   \n",
       "4          93.994          -36.4      4.857  ...    0    0    1    0    0   \n",
       "\n",
       "   sep  mon  thu  tue  wed  \n",
       "0    0    1    0    0    0  \n",
       "1    0    1    0    0    0  \n",
       "2    0    1    0    0    0  \n",
       "3    0    1    0    0    0  \n",
       "4    0    1    0    0    0  \n",
       "\n",
       "[5 rows x 36 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Display Sample data\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(41188, 36)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check data size\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    36548\n",
       "1     4640\n",
       "Name: y, dtype: int64"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check dependent value count to see if data is imbalanced\n",
    "df['y'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random Forest is a tree-based model and hence does not require feature scaling. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build base model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's seperate dependent and independent variables\n",
    "X= df.drop(columns = ['y'])\n",
    "y = df['y']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get feature list\n",
    "feature_name = X.columns.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(41188, 35)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Shape of independent variable or features\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(28831, 35)\n"
     ]
    }
   ],
   "source": [
    "# Split data to train and test and check size of train data.\n",
    "# Using 70-/30 split with random state as 420 (hyper parameter)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=140, test_size=0.30)\n",
    "\n",
    "# Print shape of train data\n",
    "print(X_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(28831, 35)\n",
      "(12357, 35)\n"
     ]
    }
   ],
   "source": [
    "# Check the shape of features\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model instance\n",
    "model_base = RandomForestClassifier(n_estimators=200)\n",
    "\n",
    "# Fit model\n",
    "model_base.fit(X_train, y_train)\n",
    "\n",
    "# Predict\n",
    "pred_base= model_base.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scores and accurancy\n",
    "model_base_score = round(model_base.score(X_train, y_train)*100,3)\n",
    "model_base_accuracy = round(model_base.score(X_test, y_test)*100,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Base model training model score:  100.0\n",
      "Base model testing model score:  91.236\n"
     ]
    }
   ],
   "source": [
    "print(\"Base model training model score: \",model_base_score)\n",
    "print(\"Base model testing model score: \",model_base_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score 0.9123573682932751\n",
      "[[10603   374]\n",
      " [  709   671]]\n",
      "Classification report  \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.97      0.95     10977\n",
      "           1       0.64      0.49      0.55      1380\n",
      "\n",
      "    accuracy                           0.91     12357\n",
      "   macro avg       0.79      0.73      0.75     12357\n",
      "weighted avg       0.90      0.91      0.91     12357\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Print matrix results\n",
    "print(\"Accuracy score %s\" %accuracy_score(y_test,pred_base))\n",
    "print(confusion_matrix(y_test, pred_base))\n",
    "print(\"Classification report  \\n %s\" %(classification_report(y_test, pred_base)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### As data is imbalanced, F1 score is important. F1 score for class 1 value prediction is very low compared to class 0 prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let' use different methods to handle imbalnced data and test model results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "def randForest(X_train, y_train, X_test, y_test):\n",
    "    \n",
    "    # Build model\n",
    "    model = RandomForestClassifier(n_estimators = 200, oob_score = True, n_jobs = -1,random_state =50, \n",
    "                              max_features = \"log2\", min_samples_leaf = 100)\n",
    "    \n",
    "    # Fir model\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # Scores and accurancy\n",
    "    model_train_score = round(model.score(X_train, y_train)*100,3)\n",
    "    model_test_score = round(model.score(X_test, y_test)*100,3)\n",
    "\n",
    "    print(\"\\nTraining model score: \",model_train_score)\n",
    "    print(\"Testing model score: \",model_test_score)\n",
    "    \n",
    "    # Get accurancy\n",
    "    accuracy = model.score(X_test, y_test)\n",
    "    print(\"\\nAccuracy\", accuracy, \"\\n\")\n",
    "    \n",
    "    # Get predictions\n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "    # Get confusion matrix\n",
    "    rnf_matri = classification_report(y_test,y_pred)\n",
    "    print(rnf_matri)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get Model results\n",
    "def getModelResult(X,y):\n",
    "    \n",
    "    # Split data into train-test -- statify y will insure there will be eqal represenation for each class\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=15, stratify=y)\n",
    "    \n",
    "    # Get model results \n",
    "    randForest(X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def featureSelection(X,y,method):\n",
    "    # Split data into train-test -- statify y will insure there will be eqal represenation for each class\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=15, stratify=y)\n",
    "    \n",
    "    # Get Random Forest Regressor instance and fit training data\n",
    "    rf = RandomForestRegressor(n_estimators=200)\n",
    "    rf.fit(X_train, y_train)\n",
    "    \n",
    "    if method == 1:\n",
    "        # Using RF to select features\n",
    "        sorted_idx = rf.feature_importances_.argsort()\n",
    "        \n",
    "        # Select X and y\n",
    "        X=X[X.columns[rf.feature_importances_>0.001]]\n",
    "        \n",
    "        # Get results witout feature selection\n",
    "        print(\"\\n-- Model performace after Random Forest feature selection -- \")\n",
    "        getModelResult(X,y)\n",
    "    \n",
    "    elif method==2:\n",
    "        \n",
    "        # Get feature importance\n",
    "        perm_importance = permutation_importance(rf, X_test, y_test)\n",
    "\n",
    "        # Get Index of feature\n",
    "        sorted_idx = perm_importance.importances_mean.argsort()\n",
    "\n",
    "        # Recreate dependent and independent data set using selected features\n",
    "        X=X[X.columns[perm_importance.importances_mean>0]]\n",
    "\n",
    "        # Get results witout feature selection\n",
    "        print(\"\\n-- Model performace after Permutation feature selection -- \")\n",
    "        getModelResult(X,y)\n",
    "    \n",
    "    elif method==3:\n",
    "        explainer = shap.TreeExplainer(rf)\n",
    "        shap_values = explainer.shap_values(X_test)\n",
    "        \n",
    "        # Get feature names\n",
    "        vals= np.abs(shap_values).mean(0)\n",
    "        feature_importance = pd.DataFrame(list(zip(X.columns,vals)),columns=['col_name','feature_importance_vals'])\n",
    "        feature_importance.sort_values(by=['feature_importance_vals'],ascending=False,inplace=True)\n",
    "        feature_importance = feature_importance[feature_importance.feature_importance_vals > 0.002 ]\n",
    "        \n",
    "        # Recreate dependent and independent data set using selected features\n",
    "        X= X[feature_importance.col_name]\n",
    "\n",
    "        # Get results witout feature selection\n",
    "        print(\"\\n -- Model performace after SHAP feature selection -- \")\n",
    "        getModelResult(X,y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Method 1: Undersampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Class count\n",
    "count_class_0, count_class_1 = df.y.value_counts()\n",
    "\n",
    "# Divide by class\n",
    "df_class_0 = df[df['y'] == 0]\n",
    "df_class_1 = df[df['y'] == 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random under-sampling:\n",
      "1    4640\n",
      "0    4640\n",
      "Name: y, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Undersample 0-class and concat the DataFrames of both class\n",
    "df_class_0_under = df_class_0.sample(count_class_1)\n",
    "df_test_under = pd.concat([df_class_0_under, df_class_1], axis=0)\n",
    "\n",
    "print('Random under-sampling:')\n",
    "print(df_test_under.y.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get features and dependent variable data \n",
    "X = df_test_under.drop('y',axis='columns')\n",
    "y = df_test_under['y']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- Model performace without feature selection -- \n",
      "\n",
      "Training model score:  86.692\n",
      "Testing model score:  87.015\n",
      "\n",
      "Accuracy 0.8701508620689655 \n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.82      0.86       928\n",
      "           1       0.84      0.92      0.88       928\n",
      "\n",
      "    accuracy                           0.87      1856\n",
      "   macro avg       0.87      0.87      0.87      1856\n",
      "weighted avg       0.87      0.87      0.87      1856\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Get results witout feature selection\n",
    "print(\"-- Model performace without feature selection -- \")\n",
    "getModelResult(X,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model without feature selection with undersampling imporved overall F1 score. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "-- Model performace after Random Forest feature selection -- \n",
      "\n",
      "Training model score:  86.503\n",
      "Testing model score:  87.015\n",
      "\n",
      "Accuracy 0.8701508620689655 \n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.83      0.86       928\n",
      "           1       0.84      0.91      0.88       928\n",
      "\n",
      "    accuracy                           0.87      1856\n",
      "   macro avg       0.87      0.87      0.87      1856\n",
      "weighted avg       0.87      0.87      0.87      1856\n",
      "\n",
      "\n",
      "-- Model performace after Permutation feature selection -- \n",
      "\n",
      "Training model score:  87.527\n",
      "Testing model score:  87.608\n",
      "\n",
      "Accuracy 0.8760775862068966 \n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.82      0.87       928\n",
      "           1       0.84      0.93      0.88       928\n",
      "\n",
      "    accuracy                           0.88      1856\n",
      "   macro avg       0.88      0.88      0.88      1856\n",
      "weighted avg       0.88      0.88      0.88      1856\n",
      "\n",
      "\n",
      " -- Model performace after SHAP feature selection -- \n",
      "\n",
      "Training model score:  87.231\n",
      "Testing model score:  87.5\n",
      "\n",
      "Accuracy 0.875 \n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.82      0.87       928\n",
      "           1       0.84      0.93      0.88       928\n",
      "\n",
      "    accuracy                           0.88      1856\n",
      "   macro avg       0.88      0.88      0.87      1856\n",
      "weighted avg       0.88      0.88      0.87      1856\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Get Model performance for undersampled data using feature selection\n",
    "featureSelection(X,y,1)\n",
    "featureSelection(X,y,2)\n",
    "featureSelection(X,y,3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Under sampling gave much better F1 score comapred wth base model. Feature selection did not add much value in improving model accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Method 2: Over Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random over-sampling:\n",
      "1    36548\n",
      "0    36548\n",
      "Name: y, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Oversample 1-class and concat the DataFrames of both classes\n",
    "df_class_1_over = df_class_1.sample(count_class_0, replace=True)\n",
    "df_test_over = pd.concat([df_class_0, df_class_1_over], axis=0)\n",
    "\n",
    "print('Random over-sampling:')\n",
    "print(df_test_over.y.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get features and dependent variable data \n",
    "X = df_test_over.drop('y',axis='columns')\n",
    "y = df_test_over['y']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- Model performace without feature selection -- \n",
      "\n",
      "Training model score:  89.254\n",
      "Testing model score:  89.295\n",
      "\n",
      "Accuracy 0.8929548563611491 \n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.83      0.89      7310\n",
      "           1       0.85      0.96      0.90      7310\n",
      "\n",
      "    accuracy                           0.89     14620\n",
      "   macro avg       0.90      0.89      0.89     14620\n",
      "weighted avg       0.90      0.89      0.89     14620\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Get model results for oversampled data\n",
    "print(\"-- Model performace without feature selection -- \")\n",
    "getModelResult(X,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model results imporved slightly with data oversampling technique. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "-- Model performace after Random Forest feature selection -- \n",
      "\n",
      "Training model score:  89.226\n",
      "Testing model score:  89.323\n",
      "\n",
      "Accuracy 0.8932284541723666 \n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.83      0.89      7310\n",
      "           1       0.85      0.96      0.90      7310\n",
      "\n",
      "    accuracy                           0.89     14620\n",
      "   macro avg       0.90      0.89      0.89     14620\n",
      "weighted avg       0.90      0.89      0.89     14620\n",
      "\n",
      "\n",
      "-- Model performace after Permutation feature selection -- \n",
      "\n",
      "Training model score:  89.055\n",
      "Testing model score:  89.159\n",
      "\n",
      "Accuracy 0.8915868673050615 \n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.83      0.88      7310\n",
      "           1       0.85      0.96      0.90      7310\n",
      "\n",
      "    accuracy                           0.89     14620\n",
      "   macro avg       0.90      0.89      0.89     14620\n",
      "weighted avg       0.90      0.89      0.89     14620\n",
      "\n",
      "\n",
      " -- Model performace after SHAP feature selection -- \n",
      "\n",
      "Training model score:  89.379\n",
      "Testing model score:  89.405\n",
      "\n",
      "Accuracy 0.8940492476060191 \n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.83      0.89      7310\n",
      "           1       0.85      0.95      0.90      7310\n",
      "\n",
      "    accuracy                           0.89     14620\n",
      "   macro avg       0.90      0.89      0.89     14620\n",
      "weighted avg       0.90      0.89      0.89     14620\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Get Model performance for oversampled data using feature selection\n",
    "featureSelection(X,y,1)\n",
    "featureSelection(X,y,2)\n",
    "featureSelection(X,y,3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Over sampling gave much better F1 score comapred wth base model. Feature selection did not add much value in improving model accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Method 3: SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop('y',axis='columns')\n",
    "y = df['y']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    36548\n",
       "0    36548\n",
       "Name: y, dtype: int64"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "# Get smote object\n",
    "smote = SMOTE(sampling_strategy='minority')\n",
    "\n",
    "# Fit data\n",
    "X_smote, y_smote = smote.fit_sample(X, y)\n",
    "\n",
    "# Check SMOTE results \n",
    "y_smote.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- Model performace without feature selection -- \n",
      "\n",
      "Training model score:  92.009\n",
      "Testing model score:  91.703\n",
      "\n",
      "Accuracy 0.91703146374829 \n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.88      0.91      7310\n",
      "           1       0.89      0.95      0.92      7310\n",
      "\n",
      "    accuracy                           0.92     14620\n",
      "   macro avg       0.92      0.92      0.92     14620\n",
      "weighted avg       0.92      0.92      0.92     14620\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Get model results for oversampled data\n",
    "print(\"-- Model performace without feature selection -- \")\n",
    "getModelResult(X_smote, y_smote)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model imporved overall F1 score even wothout feature selection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "-- Model performace after Random Forest feature selection -- \n",
      "\n",
      "Training model score:  91.961\n",
      "Testing model score:  91.676\n",
      "\n",
      "Accuracy 0.9167578659370725 \n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.88      0.91      7310\n",
      "           1       0.89      0.95      0.92      7310\n",
      "\n",
      "    accuracy                           0.92     14620\n",
      "   macro avg       0.92      0.92      0.92     14620\n",
      "weighted avg       0.92      0.92      0.92     14620\n",
      "\n",
      "\n",
      "-- Model performace after Permutation feature selection -- \n",
      "\n",
      "Training model score:  92.132\n",
      "Testing model score:  91.929\n",
      "\n",
      "Accuracy 0.9192886456908345 \n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.88      0.92      7310\n",
      "           1       0.89      0.96      0.92      7310\n",
      "\n",
      "    accuracy                           0.92     14620\n",
      "   macro avg       0.92      0.92      0.92     14620\n",
      "weighted avg       0.92      0.92      0.92     14620\n",
      "\n",
      "\n",
      " -- Model performace after SHAP feature selection -- \n",
      "\n",
      "Training model score:  92.036\n",
      "Testing model score:  91.874\n",
      "\n",
      "Accuracy 0.9187414500683995 \n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.88      0.92      7310\n",
      "           1       0.89      0.96      0.92      7310\n",
      "\n",
      "    accuracy                           0.92     14620\n",
      "   macro avg       0.92      0.92      0.92     14620\n",
      "weighted avg       0.92      0.92      0.92     14620\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Get Model performance for SMOTE data using feature selection\n",
    "featureSelection(X_smote, y_smote,1)\n",
    "featureSelection(X_smote, y_smote,2)\n",
    "featureSelection(X_smote, y_smote,3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SMOTE model gave much better F1 score comapred wth base model. Feature selection did not add much value in improving model accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Method 4: Use of Ensemble with undersampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get features and dependent variable data \n",
    "X = df.drop('y',axis='columns')\n",
    "y = df['y']\n",
    "\n",
    "# Split data into train-test -- statify y will insure there will be eqal represenation for each class\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=15, stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    29238\n",
       "1     3712\n",
       "Name: y, dtype: int64"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7.876616379310345"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "29238/3712  # We can build 8 models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "29702"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "25990+3712"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "model1 --> class1(3712) + class0(0, 3712)\n",
    "\n",
    "model2 --> class1(3712) + class0(3713, 7425)\n",
    "\n",
    "model3 --> class1(3712) + class0(7425, 11137)\n",
    "\n",
    "model4 --> class1(3712) + class0(11138, 14850)\n",
    "\n",
    "model5 --> class1(3712) + class0(14851, 18563)\n",
    "\n",
    "model6 --> class1(3712) + class0(18564, 22276)\n",
    "\n",
    "model7 --> class1(3712) + class0(22277, 25989)\n",
    "\n",
    "model8 --> class1(3712) + class0(25990, 29238)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_train_batch(df_majority, df_minority, start, end):\n",
    "    df_train = pd.concat([df_majority[start:end], df_minority], axis=0)\n",
    "    X_train = df_train.drop('y', axis='columns')\n",
    "    y_train = df_train.y\n",
    "    return X_train, y_train "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preapre df for training data set\n",
    "df_train = X_train.copy()\n",
    "df_train['y'] = y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get class level data\n",
    "df_class0 = df_train[df_train.y==0]\n",
    "df_class1 = df_train[df_train.y==1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's build 9 models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model 1---\n",
    "\n",
    "# Train test split\n",
    "X_train, y_train = get_train_batch(df_class0, df_class1, 0, 3712)\n",
    "\n",
    "# Build model\n",
    "model = RandomForestClassifier(n_estimators=200)\n",
    "\n",
    "# Fit model\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Get predictions\n",
    "y_pred1 = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model 2---3713, 7425\n",
    "\n",
    "# Train test split\n",
    "X_train, y_train = get_train_batch(df_class0, df_class1, 3713, 7425)\n",
    "\n",
    "# Build model\n",
    "model = RandomForestClassifier(n_estimators=200)\n",
    "\n",
    "# Fit model\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Get predictions\n",
    "y_pred2 = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model 3---7425, 11137\n",
    "\n",
    "# Train test split\n",
    "X_train, y_train = get_train_batch(df_class0, df_class1, 7425, 11137)\n",
    "\n",
    "# Build model\n",
    "model = RandomForestClassifier(n_estimators=200)\n",
    "\n",
    "# Fit model\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Get predictions\n",
    "y_pred3 = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model 4---11138, 14850\n",
    "\n",
    "# Train test split\n",
    "X_train, y_train = get_train_batch(df_class0, df_class1, 11138, 14850)\n",
    "\n",
    "# Build model\n",
    "model = RandomForestClassifier(n_estimators=200)\n",
    "\n",
    "# Fit model\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Get predictions\n",
    "y_pred4 = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model 5---14851, 18563\n",
    "\n",
    "# Train test split\n",
    "X_train, y_train = get_train_batch(df_class0, df_class1,14851, 18563)\n",
    "\n",
    "# Build model\n",
    "model = RandomForestClassifier(n_estimators=200)\n",
    "\n",
    "# Fit model\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Get predictions\n",
    "y_pred5 = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model 6---18564, 22276\n",
    "\n",
    "# Train test split\n",
    "X_train, y_train = get_train_batch(df_class0, df_class1,18564, 22276)\n",
    "\n",
    "# Build model\n",
    "model = RandomForestClassifier(n_estimators=200)\n",
    "\n",
    "# Fit model\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Get predictions\n",
    "y_pred6 = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model 7---18564, 22276\n",
    "\n",
    "# Train test split\n",
    "X_train, y_train = get_train_batch(df_class0, df_class1,22277, 25989)\n",
    "\n",
    "# Build model\n",
    "model = RandomForestClassifier(n_estimators=200)\n",
    "\n",
    "# Fit model\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Get predictions\n",
    "y_pred7 = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model 8---25990, 29238\n",
    "\n",
    "# Train test split\n",
    "X_train, y_train = get_train_batch(df_class0, df_class1,25990, 29238)\n",
    "\n",
    "# Build model\n",
    "model = RandomForestClassifier(n_estimators=200)\n",
    "\n",
    "# Fit model\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Get predictions\n",
    "y_pred8 = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get check all predictions\n",
    "y_pred_final =y_pred1.copy()\n",
    "\n",
    "# Loop through results\n",
    "for i in range(len(y_pred1)):\n",
    "    # Get results from all models\n",
    "    n_ones = y_pred1[i] + y_pred2[i] + y_pred3[i]+ y_pred4[i] + y_pred5[i] + y_pred6[i]+ y_pred7[i] + y_pred8[i] \n",
    "    # If sum is more than 1 means there is misclassification else classified correctly\n",
    "    if n_ones>1:\n",
    "        y_pred_final[i] = 1\n",
    "    else:\n",
    "        y_pred_final[i] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.82      0.90      7310\n",
      "           1       0.40      0.96      0.57       928\n",
      "\n",
      "    accuracy                           0.84      8238\n",
      "   macro avg       0.70      0.89      0.73      8238\n",
      "weighted avg       0.93      0.84      0.86      8238\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Get classification report\n",
    "rnf_matrix = classification_report(y_test, y_pred_final)\n",
    "print(rnf_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at all models with feature selection, we will consider test_selectFeature_score =91.929 of RF model with permutation feature selection method of feature selection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_selectFeature_score = 91.929"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cross-validation is not necessary for random forest, because multiple bagging in process of training random forest prevents over-fitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(73096, 73096)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's use smote data for grid search\n",
    "len(X_smote),len(y_smote)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test(X,y):\n",
    "    # Split data into train-test -- statify y will insure there will be equal represenation for each class\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=50, stratify=y)\n",
    "    return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into train-test -- statify y will insure there will be equal represenation for each class\n",
    "X_train, X_test, y_train, y_test = train_test(X_smote, y_smote)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'n_estimators': [200, 400, 600, 800, 1000, 1200, 1400, 1600, 1800, 2000], 'max_features': ['auto', 'sqrt'], 'max_depth': [10, 20, 30, 40, 50, 60, 70, 80, 90, 100, 110, None], 'min_samples_split': [2, 5, 10], 'min_samples_leaf': [1, 2, 4], 'bootstrap': [True, False]}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "# Number of trees in random forest\n",
    "n_estimators = [int(x) for x in np.linspace(start = 200, stop = 2000, num = 10)]\n",
    "\n",
    "# Number of features to consider at every split\n",
    "max_features = ['auto', 'sqrt']\n",
    "\n",
    "# Maximum number of levels in tree\n",
    "max_depth = [int(x) for x in np.linspace(10, 110, num = 11)]\n",
    "max_depth.append(None)\n",
    "\n",
    "# Minimum number of samples required to split a node\n",
    "min_samples_split = [2, 5, 10]\n",
    "\n",
    "# Minimum number of samples required at each leaf node\n",
    "min_samples_leaf = [1, 2, 4]\n",
    "\n",
    "# Method of selecting samples for training each tree\n",
    "bootstrap = [True, False]\n",
    "\n",
    "# Create the random grid\n",
    "random_grid = {'n_estimators': n_estimators,\n",
    "               'max_features': max_features,\n",
    "               'max_depth': max_depth,\n",
    "               'min_samples_split': min_samples_split,\n",
    "               'min_samples_leaf': min_samples_leaf,\n",
    "               'bootstrap': bootstrap}\n",
    "\n",
    "print(random_grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 100 candidates, totalling 1000 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(cv=10,\n",
       "                   estimator=RandomForestClassifier(n_estimators=200,\n",
       "                                                    random_state=42),\n",
       "                   n_iter=100, n_jobs=-1,\n",
       "                   param_distributions={'bootstrap': [True, False],\n",
       "                                        'max_depth': [10, 20, 30, 40, 50, 60,\n",
       "                                                      70, 80, 90, 100, 110,\n",
       "                                                      None],\n",
       "                                        'max_features': ['auto', 'sqrt'],\n",
       "                                        'min_samples_leaf': [1, 2, 4],\n",
       "                                        'min_samples_split': [2, 5, 10],\n",
       "                                        'n_estimators': [200, 400, 600, 800,\n",
       "                                                         1000, 1200, 1400, 1600,\n",
       "                                                         1800, 2000]},\n",
       "                   random_state=42, return_train_score=True,\n",
       "                   scoring='neg_mean_absolute_error', verbose=2)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Use the random grid to search for best hyperparameters\n",
    "\n",
    "# First create the base model to tune\n",
    "rf = RandomForestClassifier(n_estimators = 200, random_state = 42)\n",
    "\n",
    "# Random search of parameters, using 5 fold cross validation, search across 100 different combinations \n",
    "rf_random = RandomizedSearchCV(estimator=rf, param_distributions=random_grid,\n",
    "                              n_iter = 100, scoring='neg_mean_absolute_error', \n",
    "                              cv = 10, verbose=2, random_state=42, n_jobs=-1,\n",
    "                              return_train_score=True)\n",
    "\n",
    "# Fit the random search model\n",
    "rf_random.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'n_estimators': 400,\n",
       " 'min_samples_split': 2,\n",
       " 'min_samples_leaf': 1,\n",
       " 'max_features': 'sqrt',\n",
       " 'max_depth': None,\n",
       " 'bootstrap': False}"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf_random.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model with optimal parameters\n",
    "grid_search=RandomForestClassifier(max_depth= None, max_features= 'sqrt', min_samples_leaf= 1, \n",
    "                            min_samples_split= 2, n_estimators= 400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(max_features='sqrt', n_estimators=400)"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fit the data\n",
    "grid_search.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get prediction and test model accuracy\n",
    "y_pred_tuned = grid_search.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search_training_score = round(grid_search.score(X_train, y_train)*100,3)\n",
    "grid_search_tuned_accuracy = round(grid_search.score(X_test, y_test)*100,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for Random Forest on CV data: 0.9502735978112176\n",
      "Classification report  \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.94      0.95      7310\n",
      "           1       0.94      0.96      0.95      7310\n",
      "\n",
      "    accuracy                           0.95     14620\n",
      "   macro avg       0.95      0.95      0.95     14620\n",
      "weighted avg       0.95      0.95      0.95     14620\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Print matrix results\n",
    "print(\"Accuracy for Random Forest on CV data: %s\" %accuracy_score(y_test,y_pred_tuned))\n",
    "print(\"Classification report  \\n %s\" %(classification_report(y_test, y_pred_tuned)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing model score (Tuned model):  95.027\n"
     ]
    }
   ],
   "source": [
    "# Get train and test scores\n",
    "training_scoreTuned = round(rf_random.score(X_train, y_train)*100,3)\n",
    "test_scoreTuned = round(accuracy_score(y_test, y_pred_tuned)*100,3)\n",
    "print(\"Testing model score (Tuned model): \",test_scoreTuned)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tuned model gave the best results for accuracy and F1 score. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Base Model</th>\n",
       "      <td>91.236%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Select Features Model</th>\n",
       "      <td>91.929%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Tuned Model</th>\n",
       "      <td>95.027%</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      Accuracy\n",
       "                              \n",
       "Base Model             91.236%\n",
       "Select Features Model  91.929%\n",
       "Tuned Model            95.027%"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save results in csv file\n",
    "evaluations = {'': ['Base Model', 'Select Features Model', 'Tuned Model'],\n",
    "                    'Accuracy': [f\"{model_base_accuracy}%\", f\"{test_selectFeature_score}%\", f\"{test_scoreTuned}%\"]}\n",
    "\n",
    "evaluations_df = pd.DataFrame(evaluations)\n",
    "evaluations_df = evaluations_df.set_index('')\n",
    "\n",
    "# Export model result data\n",
    "evaluations_df.to_csv(model_result)\n",
    "\n",
    "# Show model results\n",
    "evaluations_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model \n",
    "pickle.dump(grid_search, open(model_file_name, 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary: \n",
    "- F1-Score is the weighted average of Precision and Recall used in all types of classification algorithms. Therefore, this score takes both false positives and false negatives into account. F1-Score is usually more useful than accuracy, especially if you have an uneven class distribution.\n",
    "- Base model accuracy of  91.24% which is slighly imporved to 91.93% with feature selection and handling imbalnced data using SMOTE technique which was further improved to 95.027 with GridSearch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernel_info": {
   "name": "dev"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "nteract": {
   "version": "0.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
