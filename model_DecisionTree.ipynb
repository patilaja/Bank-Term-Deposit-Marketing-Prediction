{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Tree Model Apprach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update sklearn to prevent version mismatches\n",
    "#!pip install sklearn --upgrade"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read the CSV and Perform Basic Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import depedencies\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import pickle\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load libraries\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report,confusion_matrix,accuracy_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "#from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import MinMaxScaler,LabelEncoder\n",
    "from sklearn.feature_selection import VarianceThreshold,SelectFromModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# File path \n",
    "data_feature_file = os.path.join(\"\",\"data\",\"featureData\",\"feature_dataframe.csv\")\n",
    "model_result = os.path.join(\"\",\"data\",\"results\",\"DecisionTree.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read data\n",
    "df = pd.read_csv(data_feature_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>job</th>\n",
       "      <th>education</th>\n",
       "      <th>contact</th>\n",
       "      <th>duration</th>\n",
       "      <th>campaign</th>\n",
       "      <th>emp.var.rate</th>\n",
       "      <th>cons.price.idx</th>\n",
       "      <th>cons.conf.idx</th>\n",
       "      <th>euribor3m</th>\n",
       "      <th>...</th>\n",
       "      <th>jun</th>\n",
       "      <th>mar</th>\n",
       "      <th>may</th>\n",
       "      <th>nov</th>\n",
       "      <th>oct</th>\n",
       "      <th>sep</th>\n",
       "      <th>mon</th>\n",
       "      <th>thu</th>\n",
       "      <th>tue</th>\n",
       "      <th>wed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>56</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>261</td>\n",
       "      <td>1</td>\n",
       "      <td>1.1</td>\n",
       "      <td>93.994</td>\n",
       "      <td>-36.4</td>\n",
       "      <td>4.857</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>57</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>149</td>\n",
       "      <td>1</td>\n",
       "      <td>1.1</td>\n",
       "      <td>93.994</td>\n",
       "      <td>-36.4</td>\n",
       "      <td>4.857</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>37</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>226</td>\n",
       "      <td>1</td>\n",
       "      <td>1.1</td>\n",
       "      <td>93.994</td>\n",
       "      <td>-36.4</td>\n",
       "      <td>4.857</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>40</td>\n",
       "      <td>10</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>151</td>\n",
       "      <td>1</td>\n",
       "      <td>1.1</td>\n",
       "      <td>93.994</td>\n",
       "      <td>-36.4</td>\n",
       "      <td>4.857</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>56</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>307</td>\n",
       "      <td>1</td>\n",
       "      <td>1.1</td>\n",
       "      <td>93.994</td>\n",
       "      <td>-36.4</td>\n",
       "      <td>4.857</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 36 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   age  job  education  contact  duration  campaign  emp.var.rate  \\\n",
       "0   56    4          2        0       261         1           1.1   \n",
       "1   57    7          5        0       149         1           1.1   \n",
       "2   37    7          5        0       226         1           1.1   \n",
       "3   40   10          3        0       151         1           1.1   \n",
       "4   56    7          5        0       307         1           1.1   \n",
       "\n",
       "   cons.price.idx  cons.conf.idx  euribor3m  ...  jun  mar  may  nov  oct  \\\n",
       "0          93.994          -36.4      4.857  ...    0    0    1    0    0   \n",
       "1          93.994          -36.4      4.857  ...    0    0    1    0    0   \n",
       "2          93.994          -36.4      4.857  ...    0    0    1    0    0   \n",
       "3          93.994          -36.4      4.857  ...    0    0    1    0    0   \n",
       "4          93.994          -36.4      4.857  ...    0    0    1    0    0   \n",
       "\n",
       "   sep  mon  thu  tue  wed  \n",
       "0    0    1    0    0    0  \n",
       "1    0    1    0    0    0  \n",
       "2    0    1    0    0    0  \n",
       "3    0    1    0    0    0  \n",
       "4    0    1    0    0    0  \n",
       "\n",
       "[5 rows x 36 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Display Sample data\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(41188, 36)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check data size\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's seperate dependent and independent variables\n",
    "X= df.drop(columns = ['y'])\n",
    "y = df['y']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get feature list\n",
    "feature_name = X.columns.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(41188, 35)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Shape of independent variable or features\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-processing model performace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32950, 35)\n"
     ]
    }
   ],
   "source": [
    "# Split data to train and test and check size of train data.\n",
    "# Using 80-/20 split with random state as 420 (hyper parameter)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=50, test_size=0.2)\n",
    "\n",
    "# Print shape of train data\n",
    "print(X_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decision trees and ensemble methods do not require feature scaling to be performed as they are not sensitive to the the variance in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build Model\n",
    "base_model = DecisionTreeClassifier(criterion='gini',max_depth=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeClassifier(max_depth=12)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train model\n",
    "base_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict \n",
    "pred_base = base_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get train and test scores\n",
    "training_score = round(base_model.score(X_train, y_train)*100,3)\n",
    "test_score = round(accuracy_score(y_test, pred_base)*100,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8973051711580481\n",
      "[[6892  396]\n",
      " [ 450  500]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.95      0.94      7288\n",
      "           1       0.56      0.53      0.54       950\n",
      "\n",
      "    accuracy                           0.90      8238\n",
      "   macro avg       0.75      0.74      0.74      8238\n",
      "weighted avg       0.89      0.90      0.90      8238\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Evaluate predictions\n",
    "print(accuracy_score(y_test, pred_base))\n",
    "print(confusion_matrix(y_test, pred_base))\n",
    "print(classification_report(y_test, pred_base))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Data Score: 95.53 %\n",
      "Testing Data Score: 89.731 %\n"
     ]
    }
   ],
   "source": [
    "print(f\"Training Data Score: {training_score} %\")\n",
    "print(f\"Testing Data Score: {test_score} %\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training results and test results are not matching. F1 score for class 1 is not comparable with class 0. This means model is biased towards class 0. In short, current state of model will not give desired results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let' use different methods to handle imbalnced data and test model results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "def decisionTree(X_train, y_train, X_test, y_test):\n",
    "    \n",
    "    # Build model\n",
    "    model = DecisionTreeClassifier(criterion='gini',max_depth=12)\n",
    "    \n",
    "    # Fit model\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # Scores and accurancy\n",
    "    model_train_score = round(model.score(X_train, y_train)*100,3)\n",
    "    model_test_score = round(model.score(X_test, y_test)*100,3)\n",
    "\n",
    "    print(\"\\nTraining model score: \",model_train_score)\n",
    "    print(\"Testing model score: \",model_test_score)\n",
    "    \n",
    "    # Get accurancy\n",
    "    accuracy = model.score(X_test, y_test)\n",
    "    print(\"\\nAccuracy\", accuracy, \"\\n\")\n",
    "    \n",
    "    # Get predictions\n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "    # Get confusion matrix\n",
    "    dt_matri = classification_report(y_test,y_pred)\n",
    "    print(dt_matri)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get Model results\n",
    "def getModelResult(X,y):\n",
    "    \n",
    "    # Split data into train-test -- statify y will insure there will be eqal represenation for each class\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=15, stratify=y)\n",
    "    \n",
    "    # Get model results \n",
    "    decisionTree(X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.inspection import permutation_importance\n",
    "import shap\n",
    "\n",
    "def featureSelection(X,y,method):\n",
    "    # Split data into train-test -- statify y will insure there will be eqal represenation for each class\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=50, stratify=y)\n",
    "    \n",
    "    # Get Random Forest Regressor instance and fit training data\n",
    "    dt = DecisionTreeRegressor()\n",
    "    dt.fit(X_train, y_train)\n",
    "    \n",
    "    if method == 1:\n",
    "        # Using DecisionTree to select features\n",
    "        sorted_idx = dt.feature_importances_.argsort()\n",
    "        \n",
    "        # Select X and y\n",
    "        X=X[X.columns[dt.feature_importances_>0.001]]\n",
    "        \n",
    "        # Get results witout feature selection\n",
    "        print(\"\\n-- Model performace after Decision Tree Regressor feature selection -- \")\n",
    "        getModelResult(X,y)\n",
    "    \n",
    "    elif method==2:\n",
    "        \n",
    "        # Get feature importance\n",
    "        perm_importance = permutation_importance(dt, X_test, y_test)\n",
    "\n",
    "        # Get Index of feature\n",
    "        sorted_idx = perm_importance.importances_mean.argsort()\n",
    "\n",
    "        # Recreate dependent and independent data set using selected features\n",
    "        X=X[X.columns[perm_importance.importances_mean>0]]\n",
    "\n",
    "        # Get results witout feature selection\n",
    "        print(\"\\n-- Model performace after Permutation feature selection -- \")\n",
    "        getModelResult(X,y)\n",
    "    \n",
    "    elif method==3:\n",
    "        explainer = shap.TreeExplainer(dt)\n",
    "        shap_values = explainer.shap_values(X_test)\n",
    "        \n",
    "        # Get feature names\n",
    "        vals= np.abs(shap_values).mean(0)\n",
    "        feature_importance = pd.DataFrame(list(zip(X.columns,vals)),columns=['col_name','feature_importance_vals'])\n",
    "        feature_importance.sort_values(by=['feature_importance_vals'],ascending=False,inplace=True)\n",
    "        feature_importance = feature_importance[feature_importance.feature_importance_vals > 0.002 ]\n",
    "        \n",
    "        # Recreate dependent and independent data set using selected features\n",
    "        X= X[feature_importance.col_name]\n",
    "\n",
    "        # Get results witout feature selection\n",
    "        print(\"\\n -- Model performace after SHAP feature selection -- \")\n",
    "        getModelResult(X,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Method 1: Undersampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Class count\n",
    "count_class_0, count_class_1 = df.y.value_counts()\n",
    "\n",
    "# Divide by class\n",
    "df_class_0 = df[df['y'] == 0]\n",
    "df_class_1 = df[df['y'] == 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random under-sampling:\n",
      "1    4640\n",
      "0    4640\n",
      "Name: y, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Undersample 0-class and concat the DataFrames of both class\n",
    "df_class_0_under = df_class_0.sample(count_class_1)\n",
    "df_test_under = pd.concat([df_class_0_under, df_class_1], axis=0)\n",
    "\n",
    "print('Random under-sampling:')\n",
    "print(df_test_under.y.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get features and dependent variable data \n",
    "X = df_test_under.drop('y',axis='columns')\n",
    "y = df_test_under['y']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- Model performace without feature selection -- \n",
      "\n",
      "Training model score:  96.538\n",
      "Testing model score:  85.614\n",
      "\n",
      "Accuracy 0.8561422413793104 \n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      0.85      0.85       928\n",
      "           1       0.85      0.87      0.86       928\n",
      "\n",
      "    accuracy                           0.86      1856\n",
      "   macro avg       0.86      0.86      0.86      1856\n",
      "weighted avg       0.86      0.86      0.86      1856\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Get results witout feature selection\n",
    "print(\"-- Model performace without feature selection -- \")\n",
    "getModelResult(X,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### F1 score is imporved compared to base model. Training results and test results are still not comparable. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "-- Model performace after Decision Tree Regressor feature selection -- \n",
      "\n",
      "Training model score:  96.525\n",
      "Testing model score:  84.806\n",
      "\n",
      "Accuracy 0.8480603448275862 \n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.85      0.85      0.85       928\n",
      "           1       0.85      0.85      0.85       928\n",
      "\n",
      "    accuracy                           0.85      1856\n",
      "   macro avg       0.85      0.85      0.85      1856\n",
      "weighted avg       0.85      0.85      0.85      1856\n",
      "\n",
      "\n",
      "-- Model performace after Permutation feature selection -- \n",
      "\n",
      "Training model score:  96.404\n",
      "Testing model score:  85.399\n",
      "\n",
      "Accuracy 0.8539870689655172 \n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      0.84      0.85       928\n",
      "           1       0.85      0.86      0.86       928\n",
      "\n",
      "    accuracy                           0.85      1856\n",
      "   macro avg       0.85      0.85      0.85      1856\n",
      "weighted avg       0.85      0.85      0.85      1856\n",
      "\n",
      "\n",
      " -- Model performace after SHAP feature selection -- \n",
      "\n",
      "Training model score:  96.471\n",
      "Testing model score:  85.075\n",
      "\n",
      "Accuracy 0.8507543103448276 \n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.85      0.85      0.85       928\n",
      "           1       0.85      0.85      0.85       928\n",
      "\n",
      "    accuracy                           0.85      1856\n",
      "   macro avg       0.85      0.85      0.85      1856\n",
      "weighted avg       0.85      0.85      0.85      1856\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Get Model performance for undersampled data using feature selection\n",
    "featureSelection(X,y,1)\n",
    "featureSelection(X,y,2)\n",
    "featureSelection(X,y,3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test and Train Model accurancy with under sampled data and feature selection gave good results but they are not close enough. F1 score for train and test is comparable. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Method 2: Over Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random over-sampling:\n",
      "1    36548\n",
      "0    36548\n",
      "Name: y, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Oversample 1-class and concat the DataFrames of both classes\n",
    "df_class_1_over = df_class_1.sample(count_class_0, replace=True)\n",
    "df_test_over = pd.concat([df_class_0, df_class_1_over], axis=0)\n",
    "\n",
    "print('Random over-sampling:')\n",
    "print(df_test_over.y.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get features and dependent variable data \n",
    "X = df_test_over.drop('y',axis='columns')\n",
    "y = df_test_over['y']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- Model performace without feature selection -- \n",
      "\n",
      "Training model score:  94.045\n",
      "Testing model score:  92.606\n",
      "\n",
      "Accuracy 0.9260601915184679 \n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.88      0.92      7310\n",
      "           1       0.89      0.97      0.93      7310\n",
      "\n",
      "    accuracy                           0.93     14620\n",
      "   macro avg       0.93      0.93      0.93     14620\n",
      "weighted avg       0.93      0.93      0.93     14620\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Get model results for oversampled data\n",
    "print(\"-- Model performace without feature selection -- \")\n",
    "getModelResult(X,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### F1 score is improved significantly comapred to base model. Test and Train Models without Feature with over sampling without feature selection gave accuracy score. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "-- Model performace after Decision Tree Regressor feature selection -- \n",
      "\n",
      "Training model score:  94.045\n",
      "Testing model score:  92.681\n",
      "\n",
      "Accuracy 0.926812585499316 \n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.88      0.92      7310\n",
      "           1       0.89      0.98      0.93      7310\n",
      "\n",
      "    accuracy                           0.93     14620\n",
      "   macro avg       0.93      0.93      0.93     14620\n",
      "weighted avg       0.93      0.93      0.93     14620\n",
      "\n",
      "\n",
      "-- Model performace after Permutation feature selection -- \n",
      "\n",
      "Training model score:  94.051\n",
      "Testing model score:  92.647\n",
      "\n",
      "Accuracy 0.9264705882352942 \n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.88      0.92      7310\n",
      "           1       0.89      0.98      0.93      7310\n",
      "\n",
      "    accuracy                           0.93     14620\n",
      "   macro avg       0.93      0.93      0.93     14620\n",
      "weighted avg       0.93      0.93      0.93     14620\n",
      "\n",
      "\n",
      " -- Model performace after SHAP feature selection -- \n",
      "\n",
      "Training model score:  94.056\n",
      "Testing model score:  92.62\n",
      "\n",
      "Accuracy 0.9261969904240767 \n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.88      0.92      7310\n",
      "           1       0.89      0.98      0.93      7310\n",
      "\n",
      "    accuracy                           0.93     14620\n",
      "   macro avg       0.93      0.93      0.93     14620\n",
      "weighted avg       0.93      0.93      0.93     14620\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Get Model performance for oversampled data using feature selection\n",
    "featureSelection(X,y,1)\n",
    "featureSelection(X,y,2)\n",
    "featureSelection(X,y,3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### F1 score is improved significantly comapred to base model. Test and Train Models with Feature with over sampling without feature selection gave accuracy score. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Method 3: SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop('y',axis='columns')\n",
    "y = df['y']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    36548\n",
       "0    36548\n",
       "Name: y, dtype: int64"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "# Get smote object\n",
    "smote = SMOTE(sampling_strategy='minority')\n",
    "\n",
    "# Fit data\n",
    "X_smote, y_smote = smote.fit_sample(X, y)\n",
    "\n",
    "# Check SMOTE results \n",
    "y_smote.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- Model performace without feature selection -- \n",
      "\n",
      "Training model score:  95.964\n",
      "Testing model score:  93.003\n",
      "\n",
      "Accuracy 0.9300273597811217 \n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.92      0.93      7310\n",
      "           1       0.92      0.94      0.93      7310\n",
      "\n",
      "    accuracy                           0.93     14620\n",
      "   macro avg       0.93      0.93      0.93     14620\n",
      "weighted avg       0.93      0.93      0.93     14620\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Get model results for oversampled data\n",
    "print(\"-- Model performace without feature selection -- \")\n",
    "getModelResult(X_smote, y_smote)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### F1 score is improved significantly comapred to base model. Test and Train Models without Feature with over sampling without feature selection gave accuracy score. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "-- Model performace after Decision Tree Regressor feature selection -- \n",
      "\n",
      "Training model score:  95.968\n",
      "Testing model score:  92.955\n",
      "\n",
      "Accuracy 0.9295485636114911 \n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.92      0.93      7310\n",
      "           1       0.92      0.94      0.93      7310\n",
      "\n",
      "    accuracy                           0.93     14620\n",
      "   macro avg       0.93      0.93      0.93     14620\n",
      "weighted avg       0.93      0.93      0.93     14620\n",
      "\n",
      "\n",
      "-- Model performace after Permutation feature selection -- \n",
      "\n",
      "Training model score:  95.98\n",
      "Testing model score:  93.071\n",
      "\n",
      "Accuracy 0.9307113543091655 \n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.92      0.93      7310\n",
      "           1       0.92      0.94      0.93      7310\n",
      "\n",
      "    accuracy                           0.93     14620\n",
      "   macro avg       0.93      0.93      0.93     14620\n",
      "weighted avg       0.93      0.93      0.93     14620\n",
      "\n",
      "\n",
      " -- Model performace after SHAP feature selection -- \n",
      "\n",
      "Training model score:  95.981\n",
      "Testing model score:  92.914\n",
      "\n",
      "Accuracy 0.9291381668946649 \n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.92      0.93      7310\n",
      "           1       0.92      0.94      0.93      7310\n",
      "\n",
      "    accuracy                           0.93     14620\n",
      "   macro avg       0.93      0.93      0.93     14620\n",
      "weighted avg       0.93      0.93      0.93     14620\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Get Model performance for SMOTE data using feature selection\n",
    "featureSelection(X_smote, y_smote,1)\n",
    "featureSelection(X_smote, y_smote,2)\n",
    "featureSelection(X_smote, y_smote,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Best model test score\n",
    "test_scoreSelect = 93.071"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### F1 score is improved significantly comapred to base model. Test and Train Models with SMOTE technique with feature selection gave accuracy score. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameter Tuning\n",
    "\n",
    "Use `GridSearchCV` to tune the model's parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(73096, 73096)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's use smote data for grid search\n",
    "len(X_smote),len(y_smote)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def featureCSVSelection(X,y):\n",
    "    # Split data into train-test -- statify y will insure there will be equal represenation for each class\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=50, stratify=y)\n",
    "    \n",
    "    # Get Decision Tree Regressor instance and fit training data\n",
    "    dt = DecisionTreeRegressor()\n",
    "    dt.fit(X_train, y_train)\n",
    "    \n",
    "    explainer = shap.TreeExplainer(dt)\n",
    "    shap_values = explainer.shap_values(X_test)\n",
    "\n",
    "    # Get feature names\n",
    "    vals= np.abs(shap_values).mean(0)\n",
    "    feature_importance = pd.DataFrame(list(zip(X.columns,vals)),columns=['col_name','feature_importance_vals'])\n",
    "    feature_importance.sort_values(by=['feature_importance_vals'],ascending=False,inplace=True)\n",
    "    feature_importance = feature_importance[feature_importance.feature_importance_vals > 0.002 ]\n",
    "\n",
    "    # Recreate dependent and independent data set using selected features\n",
    "    X= X[feature_importance.col_name]\n",
    "\n",
    "    # Get results witout feature selection\n",
    "    print(\"\\n -- Model performace after SHAP feature selection -- \")\n",
    "    getCVModelResult(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use grid search to tune the model\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "def getCVModelResult(X,y):\n",
    "    \n",
    "    # Split data into train-test -- statify y will insure there will be eqal represenation for each class\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=15, stratify=y)\n",
    "    \n",
    "\n",
    "    params = {'max_leaf_nodes': list(range(2, 100)), 'min_samples_split': [2], \n",
    "          'criterion': ['gini', 'entropy'], 'max_depth': [2,4,6,8,10,12]}\n",
    "    grid_search_cv = GridSearchCV(DecisionTreeClassifier(random_state=50), params, verbose=1, cv=5)\n",
    "    \n",
    "    \n",
    "    # Fit model\n",
    "    grid_search_cv.fit(X_train, y_train)\n",
    "    \n",
    "    # Scores and accurancy\n",
    "    model_train_score = round(grid_search_cv.score(X_train, y_train)*100,3)\n",
    "    model_test_score = round(grid_search_cv.score(X_test, y_test)*100,3)\n",
    "\n",
    "    print(\"\\nTraining model score: \",model_train_score)\n",
    "    print(\"Testing model score: \",model_test_score)\n",
    "    \n",
    "    # Get accurancy\n",
    "    accuracy = grid_search_cv.score(X_test, y_test)\n",
    "    print(\"\\nAccuracy\", accuracy, \"\\n\")\n",
    "    \n",
    "    # Get predictions\n",
    "    y_pred = grid_search_cv.predict(X_test)\n",
    "\n",
    "    # Get confusion matrix\n",
    "    csv_matri = classification_report(y_test,y_pred)\n",
    "    print(csv_matri)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " -- Model performace after SHAP feature selection -- \n",
      "Fitting 5 folds for each of 1176 candidates, totalling 5880 fits\n",
      "\n",
      "Training model score:  92.231\n",
      "Testing model score:  92.025\n",
      "\n",
      "Accuracy 0.9202462380300958 \n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.91      0.92      7310\n",
      "           1       0.91      0.93      0.92      7310\n",
      "\n",
      "    accuracy                           0.92     14620\n",
      "   macro avg       0.92      0.92      0.92     14620\n",
      "weighted avg       0.92      0.92      0.92     14620\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Get Model performance for SMOTE data using feature selection\n",
    "featureCSVSelection(X_smote, y_smote)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test model score is same regardless feature selection method\n",
    "test_scoreTuned = 92.025"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### GridSearch gave us the best model with overall accuracy of 92% and also F1 score as 92% for each class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save results in csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Base Model</th>\n",
       "      <td>89.731%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Select Features Model</th>\n",
       "      <td>93.071%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Tuned Model</th>\n",
       "      <td>92.025%</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      Accuracy\n",
       "                              \n",
       "Base Model             89.731%\n",
       "Select Features Model  93.071%\n",
       "Tuned Model            92.025%"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluations = {'': ['Base Model', 'Select Features Model', 'Tuned Model'],\n",
    "               'Accuracy': [f\"{test_score}%\", f\"{test_scoreSelect}%\", f\"{test_scoreTuned}%\"]}\n",
    "\n",
    "evaluations_df = pd.DataFrame(evaluations)\n",
    "evaluations_df = evaluations_df.set_index('')\n",
    "\n",
    "evaluations_df.to_csv(model_result)\n",
    "evaluations_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary: \n",
    "-  Decision Tree Model built using GridSearch gave the best results for each class with an accuracy of 92.025 %.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernel_info": {
   "name": "dev"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "nteract": {
   "version": "0.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
